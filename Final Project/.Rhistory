for (j in 1:K)
if (delta[T, j] == logp_zstar)
zstar[T] = j;
for (t in 1:(T - 1)) {
zstar[T - t] = bpointer[T - t + 1, zstar[T - t + 1]];
}
}
}"
#stan_model(model_code = hmm_model)
hmm_model_2 = "
functions {
vector normalize(vector x) {
return x / sum(x);
}
}
data {
int<lower=1> T; // 期數
int<lower=1> K; // state數
int y[T];       // observation
int w[T];       // weekday
//vector<lower=0>[K] alpha;  // transit prior
}
parameters {
// Discrete state model
simplex[K] pi1;  // initial state prob
simplex[K] A[K]; // transition matrix
// Poisson observation model
// Assumed lambda = a + bp
// vector[K] lambda;
vector<lower=0,upper=70>[K] a;
vector<lower=0,upper=10>[K] bp;
}
transformed parameters {
vector[K] logalpha[T];
{ // Forward algorithm log p(z_t = j | y_{1:t})
real accumulator[K];
logalpha[1] = log(pi1) + poisson_lpmf(y[1] | a + bp*w[1]);
for (t in 2:T) {
for (j in 1:K) { // j = current (t)
for (i in 1:K) { // i = previous (t-1)
// Murphy (2012) p. 609 eq. 17.48
// belief state      + transition prob + local evidence at t
accumulator[i] = logalpha[t-1, i] + log(A[i, j]) + poisson_lpmf(y[t] | a[j] + bp[j] * w[t]);
}
logalpha[t, j] = log_sum_exp(accumulator);
}
}
} // Forward
}
model {
//a ~ lognormal(mu, sigma)
a[1] ~ lognormal(3,1);
a[2] ~ lognormal(2,1);
a[3] ~ lognormal(1,1);
//b ~ normal(mu, sigma)
bp[1] ~ lognormal(1,1);
bp[2] ~ lognormal(1,1);
bp[3] ~ lognormal(1,1);
for (k in 1:K)
A[k] ~ dirichlet(rep_vector(1, K));
target += log_sum_exp(logalpha[T]); // Note: update based only on last logalpha
}
generated quantities {
vector[K] alpha[T];
vector[K] logbeta[T];
vector[K] loggamma[T];
vector[K] beta[T];
vector[K] gamma[T];
int<lower=1, upper=K> zstar[T];
real logp_zstar;
{ // Forward algortihm
for (t in 1:T)
alpha[t] = softmax(logalpha[t]);
} // Forward
{ // Backward algorithm log p(y_{t+1:T} | z_t = j)
real accumulator[K];
for (j in 1:K)
logbeta[T, j] = 1;
for (tforward in 0:(T-2)) {
int t;
t = T - tforward;
for (j in 1:K) {    // j = previous (t-1)
for (i in 1:K) {  // i = next (t)
// Murphy (2012) Eq. 17.58
// backwards t    + transition prob + local evidence at t
accumulator[i] = logbeta[t, i] + log(A[j, i]) + poisson_lpmf(y[t] | a[i] + bp[i] * w[t]);
}
logbeta[t-1, j] = log_sum_exp(accumulator);
}
}
for (t in 1:T)
beta[t] = softmax(logbeta[t]);
} // Backward
{ // forward-backward algorithm log p(z_t = j | y_{1:T})
for(t in 1:T) {
loggamma[t] = alpha[t] .* beta[t];
}
for(t in 1:T)
gamma[t] = normalize(loggamma[t]);
} // forward-backward
{ // Viterbi algorithm
int bpointer[T, K]; // backpointer to the most likely previous state on the most probable path
real delta[T, K];   // max prob for the sequence up to t
// that ends with an emission from state k
for (j in 1:K)
delta[1, K] = poisson_lpmf(y[1] | a[j] + bp[j] * w[1]);
for (t in 2:T) {
for (j in 1:K) { // j = current (t)
delta[t, j] = negative_infinity();
for (i in 1:K) { // i = previous (t-1)
real logp;
logp = delta[t-1, i] + log(A[i, j]) + poisson_lpmf(y[t] | a[j] + bp[j] * w[t]);
if (logp > delta[t, j]) {
bpointer[t, j] = i;
delta[t, j] = logp;
}
}
}
}
logp_zstar = max(delta[T]);
for (j in 1:K)
if (delta[T, j] == logp_zstar)
zstar[T] = j;
for (t in 1:(T - 1)) {
zstar[T - t] = bpointer[T - t + 1, zstar[T - t + 1]];
}
}
}"
#stan_model(model_code = hmm_model_2)
hmm_init <- function(K, y) { # K: number of stages, y: y_train
clasif <- kmeans(y, K)
init.mu <- by(y, clasif$cluster, mean)
init.sigma <- by(y, clasif$cluster, sd)
init.order <- order(init.mu)
list(
mu = init.mu[init.order], sigma = init.sigma[init.order]
) }
hmm_VI_fit = function(K,y){ # K: number of stages, y: y_train
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
stan.model = stan_model(model_code = hmm_model)
stan.data = list(
T = length(y),
K = K,
y=y)
vb(stan.model,
data = stan.data,
iter = 20000,
init = function(){hmm_init(K,y)}
)
}
fit1.1 = hmm_VI_fit(3, data$quantity)
post1.1 = as.data.frame(fit1.1)
post1.1 = post1.1 %>% select(-contains("log")) %>% select(-contains("alpha")) %>% select(-contains("beta")) %>% select(-contains("gamma"))
post_pred = data_frame(index = seq(1:721))
for(i in 1:nrow(post1.1)){
lambda_lst = post1.1[i,13:15] %>% t() %>% as.vector()
z_lst = post1.1[i,17:ncol(post1.1)-1] %>% t() %>% as.vector()
mylist = c()
for(j in 1:length(z_lst)){
mylist = append(mylist,rpois(1,lambda_lst[z_lst[j]]))
}
post_pred = post_pred %>% cbind(.,mylist)
}
post_pred = post_pred[,2:ncol(post_pred)]
a = post_pred %>% apply(1,PI)
data$pred = post_pred %>% apply(1,mean)
data$l_PI = a[1,]
data$h_PI = a[2,]
fig1.1 = data %>%
ggplot() +
geom_point(aes(x=seq(1:721), y=quantity),color = 'dodgerblue') +
geom_ribbon(aes(x=seq(1:721),
ymin = l_PI,
ymax = h_PI),
alpha = 0.7) +
geom_line(aes(x=seq(1:721), y=pred))+
ggtitle("Model1 backtest")
fit1.2 = hmm_VI_fit(3, train$quantity)
post1.2 = as.data.frame(fit1.2)
post1.2 = post1.2 %>% select(-contains("log")) %>% select(-contains("alpha")) %>% select(-contains("beta")) %>% select(-contains("gamma"))
# prediction
hmm_predict_1 = function(data, T, K){
# parameters
A = matrix(data[4:(3+K^2)], nrow = K)
lambda = as_vector(data[13:15])
Z = unlist(data[567])
# hidden path
z = vector("numeric", T+1)
z[1] = Z
for (t in 2:(T+1)){
z[t] = sample(1:K, size = 1, prob = A[z[t - 1], ])
}
# prediction
y = vector("numeric", T)
for (t in 1:T){
y[t] = rpois(1, lambda[z[t+1]])
}
list(y = y, z = z[2:length(z)])
}
pred_1 =  post1.2 %>%
apply(., MARGIN=1, FUN=hmm_predict_1, T=169, K=3) %>%
lapply(., function (x) x[c('y')]) %>%
unlist() %>%
matrix(., nrow = 169) %>%
apply(., MARGIN = 1, mean)
PI_1 =  post1.2 %>%
apply(., MARGIN=1, FUN=hmm_predict_1, T=169, K=3) %>%
lapply(., function (x) x[c('y')]) %>%
unlist() %>%
matrix(., nrow = 169) %>%
apply(., MARGIN = 1, PI)
test$pred = pred_1
test$l_PI = PI_1[1,]
test$h_PI = PI_1[2,]
fig1.2 = test %>% ggplot() +
geom_ribbon(aes(x=seq(553,721),
ymin=l_PI,
ymax=h_PI),alpha=0.7)+
geom_point(aes(x=seq(553,721), y=quantity),color = 'dodgerblue') +
geom_line(aes(x=seq(553,721),y=pred)) +
ggtitle("Model1 + Method1")
hmm_VI_fit_2 = function(K,y,w){ # K: number of stages, y: y_train
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
stan.model = stan_model(model_code = hmm_model_2)
stan.data = list(
T = length(y),
K = K,
y=y,
w=w)
vb(stan.model,
data = stan.data,
iter = 20000,
init = function(){hmm_init(K,y)}
)
}
fit_2 = hmm_VI_fit_2(3, data$quantity, as.integer(data$weekday))
post_2 = as.data.frame(fit_2)
post_2 = post_2 %>% select(-contains("log")) %>% select(-contains("alpha")) %>% select(-contains("beta")) %>% select(-contains("gamma"))
post_2_pred = data_frame(index = seq(1:721),
weekday = as.integer(data$weekday))
for(i in 1:nrow(post_2)){
z_lst = post_2[i,] %>%
select(contains("zstar")) %>%
t() %>% as.vector()
mylist = c()
for(j in 1:length(z_lst)){
lambda_lst = post_2[j,13:15] + post_2[j,16:18] * post_2_pred[[j,2]] %>% t() %>% as.vector()
mylist = append(mylist,rpois(1,lambda_lst[[z_lst[j]]]))
}
post_2_pred = post_2_pred %>% cbind(.,mylist)
}
View(wk4)
post_2_pred = post_2_pred[,3:ncol(post_2_pred)]
a = post_2_pred %>% apply(1,PI)
data$pred_2 = post_2_pred %>% apply(1,mean)
data$l_PI_2 = a[1,]
data$h_PI_2 = a[2,]
fig2.1 = data %>%
ggplot() +
geom_ribbon(aes(x=seq(1:721),
ymin = l_PI_2,
ymax = h_PI_2),
alpha = 0.7) +
geom_point(aes(x=seq(1:721), y=quantity),color = 'dodgerblue') +
geom_line(aes(x=seq(1:721), y=pred_2))+
ggtitle("Model2 Backtest")
fit2.2 = hmm_VI_fit_2(3, train$quantity, train$weekday)
post2.2 = as.data.frame(fit2.2)
post2.2 = post2.2 %>% select(-contains("log")) %>% select(-contains("alpha")) %>% select(-contains("beta")) %>% select(-contains("gamma"))
colnames(post2.2)
# prediction
hmm_predict_2 = function(data, T, K, weekday_lst){
# parameters
A = matrix(data[4:(3+K^2)], nrow = K)
Z = data[570] %>% unlist()
a = data[13:15] %>% unlist()
bp = data[16:18] %>% unlist()
# hidden path
z = vector("numeric", T+1)
z[1] = Z
for (t in 2:(T+1)){
z[t] = sample(1:K, size = 1, prob = A[z[t - 1], ])
}
y = vector("numeric", T)
for (t in 1:T){
y[t] = rpois(1, lambda = a[z[t+1]] + bp[z[t+1]]*weekday_lst[t])
}
list(y = y, z = z[2:length(z)])
}
pred_2 = post2.2 %>%
apply(., MARGIN=1, FUN=hmm_predict_2, T=169, K=3, test$weekday) %>%
lapply(., function (x) x[c('y')]) %>%
unlist() %>%
matrix(., nrow = 169) %>%
apply(., MARGIN = 1, mean)
PI_2 = post2.2 %>%
apply(., MARGIN=1, FUN=hmm_predict_2, T=169, K=3, test$weekday) %>%
lapply(., function (x) x[c('y')]) %>%
unlist() %>%
matrix(, nrow = 169) %>%
apply(., MARGIN = 1, PI)
test$pred_2 = pred_2
test$l_PI_2 = PI_2[1,]
test$h_PI_2 = PI_2[2,]
fig2.2 = test %>% ggplot() +
geom_ribbon(aes(x=seq(553,721),
ymin = l_PI_2,
ymax = h_PI_2),alpha=0.7) +
geom_point(aes(x=seq(553,721), y=quantity),color = 'dodgerblue') +
geom_line(aes(x=seq(553,721),y=pred_2))+
ggtitle("Model2 + Method1")
# RMSE comparison
cat("The RMSE of model_1 is ", RMSE(data$pred, data$quantity))
cat("The RMSE of model_2 is ", RMSE(data$pred_2, data$quantity))
fit_logalpha = as.data.frame(fit1.1) %>% select(contains('logalpha'))
fit_logalpha = fit_logalpha %>%
select(`logalpha[721,1]`,`logalpha[721,2]`,`logalpha[721,3]`)
fit_2_logalpha = as.data.frame(fit_2) %>% select(contains('logalpha'))
fit_2_logalpha = fit_2_logalpha %>%
select(`logalpha[721,1]`,`logalpha[721,2]`,`logalpha[721,3]`)
log_list <- list(fit_logalpha, fit_2_logalpha)
test <- lapply(log_list, as.matrix)
waic_list <- list()
loo_list <- list()
for(i in 1:2) {
waic_list[[i]] <- waic(test[[i]], r_eff = rel_n_eff_list[[i]], cores = 4)
loo_list[[i]] <- loo(test[[i]], cores = 4)
}
names(waic_list) <- c('fit1', 'fit2')
print(waic_list)
print(loo_list)
# graph comparison
grid.arrange(fig1.1, fig1.2, fig2.1, fig2.2, nrow = 2)
# Extract most likely path from posterior
zlst1.2 = post1.2 %>% select(contains('zstar')) %>% apply(FUN=MODE, MARGIN = 2) %>% unlist()
zlst2.2 = post2.2 %>% select(contains('zstar')) %>% apply(FUN=MODE, MARGIN = 2) %>% unlist()
train$z_1 = zlst1.2
train$z_2 = zlst2.2
train$hourly = rep(seq(1:24),23) - 1
train$date = train$index %>% str_sub(1,11)
train$dayofweek = as.POSIXlt(train$index)$wday + 1
pred_1_z = c()
for (i in seq(1:7)){
temp = train %>%
filter(dayofweek == i) %>%
select(z_1,hourly,date) %>%
spread(, key=date, value=z_1) %>%
apply(MARGIN = 1, FUN = MODE)
if (i == 1) pred_1_z = temp
else pred_1_z = append(pred_1_z,temp)
}
pred_2_z = c()
for (i in seq(1:7)){
temp = train %>%
filter(dayofweek == i) %>%
select(z_2,hourly,date) %>%
spread(, key=date, value=z_2) %>%
apply(MARGIN = 1, FUN = MODE)
if (i == 1) pred_2_z = temp
else pred_2_z = append(pred_2_z,temp)
}
test_2 = data[(24*23+1):720,]
test_2$pred_1_z = pred_1_z
test_2$pred_2_z = pred_2_z
pred_1.2 = function(z){
a = rpois(1000, post1.2[,12+z])
return(a)
}
test_2$pred_1_mean = test_2$pred_1_z %>%
mapply(FUN=pred_1.2) %>%
apply(FUN=mean,MARGIN = 2)
test_2$pred_1_lPI = test_2$pred_1_z %>%
mapply(FUN=pred_1.2) %>%
apply(FUN=PI,MARGIN = 2) %>% .[1,]
test_2$pred_1_hPI = test_2$pred_1_z %>%
mapply(FUN=pred_1.2) %>%
apply(FUN=PI,MARGIN = 2) %>% .[2,]
pred_2.2 = function(z,w){
a = rpois(1000, post2.2[,12+z]+post2.2[,15+z]*w)
return(a)
}
for (i in c(1:nrow(test_2))){
test_2$pred_2_mean[i] =
pred_2.2(test_2$pred_2_z[i], test_2$weekday[i]) %>% mean
test_2$pred_2_hPI[i] =
pred_2.2(test_2$pred_2_z[i], test_2$weekday[i]) %>%
PI %>% .[2]
test_2$pred_2_lPI[i] =
pred_2.2(test_2$pred_2_z[i], test_2$weekday[i]) %>%
PI %>% .[1]
}
test_2$x_axis = seq(1:nrow(test_2))
fig1.3 = test_2 %>% ggplot() +
geom_point(aes(x=x_axis, y=quantity), color = 'dodgerblue') +
geom_line(aes(x=x_axis, y=pred_1_mean)) +
geom_ribbon(aes(x=x_axis,
ymin = pred_1_lPI,
ymax = pred_1_hPI),
alpha = 0.7) +
ggtitle("Model1 + Method2")
fig2.3 = test_2 %>% ggplot() +
geom_point(aes(x=x_axis, y=quantity), color = 'dodgerblue') +
geom_line(aes(x=x_axis, y=pred_2_mean)) +
geom_ribbon(aes(x=x_axis,
ymin = pred_2_lPI,
ymax = pred_2_hPI),
alpha = 0.7) +
ggtitle("Model2 + Method2")
grid.arrange(fig1.2, fig1.3, fig2.2, fig2.3, nrow=2)
cat("The RMSE of model_1 + method_2 is", RMSE(test_2$pred_1_mean, test_2$quantity),"\n")
cat("The RMSE of model_2 + method_2 is", RMSE(test_2$pred_2_mean, test_2$quantity))
unlink('QBS Final Project_cache', recursive = TRUE)
train2 = train[(24*2+1):(24*23),] # only use full weeks from 1/3 ~ 1/23
for (i in (1:7)){
temp = train2 %>%
filter(dayofweek == i) %>%
select(z_1,hourly,date) %>%
spread(, key=date, value=z_1) %>%
select(-hourly)
names(temp)<-c("w1","w2","w3")
if (i == 1) pred_3_z = temp
else pred_3_z = rbind(pred_3_z,temp)
}
for (i in seq(1:7)){
temp = train2 %>%
filter(dayofweek == i) %>%
select(z_2,hourly,date) %>%
spread(, key=date, value=z_2) %>%
select(-hourly)
names(temp)<-c("w1","w2","w3")
if (i == 1) pred_4_z = temp
else pred_4_z = rbind(pred_4_z,temp)
}
pred_3_z
for (i in 1:nrow(pred_3_z)){
pred_3_z$pred_mean[i] = 1/3 * mean(
pred_1.2(pred_3_z$w1[i])+
pred_1.2(pred_3_z$w2[i])+
pred_1.2(pred_3_z$w3[i]))
pred_3_z$pred_lPI[i] = 1/3 * PI(
pred_1.2(pred_3_z$w1[i])+
pred_1.2(pred_3_z$w2[i])+
pred_1.2(pred_3_z$w3[i]))[1]
pred_3_z$pred_hPI[i] =
1/3 * PI(
pred_1.2(pred_3_z$w1[i])+
pred_1.2(pred_3_z$w2[i])+
pred_1.2(pred_3_z$w3[i]))[2]
}
pred_4_z$weekday = test_2$weekday
for (i in c(1:nrow(pred_4_z))){
pred_4_z$pred_mean[i] =
1/3 * mean(
pred_2.2(pred_4_z$w1[i], pred_4_z$weekday[i])+
pred_2.2(pred_4_z$w2[i], pred_4_z$weekday[i])+
pred_2.2(pred_4_z$w3[i], pred_4_z$weekday[i]))
pred_4_z$pred_lPI[i] =
1/3 * PI(
pred_2.2(pred_4_z$w1[i], pred_4_z$weekday[i])+
pred_2.2(pred_4_z$w2[i], pred_4_z$weekday[i])+
pred_2.2(pred_4_z$w3[i], pred_4_z$weekday[i]))[1]
pred_4_z$pred_hPI[i] =
1/3 * PI(
pred_2.2(pred_4_z$w1[i], pred_4_z$weekday[i])+
pred_2.2(pred_4_z$w2[i], pred_4_z$weekday[i])+
pred_2.2(pred_4_z$w3[i], pred_4_z$weekday[i]))[2]
}
# RMSE Comparison
pred_3_z$quantity = test_2$quantity
pred_4_z$quantity = test_2$quantity
RMSE(pred_3_z$pred_mean, pred_3_z$quantity)
RMSE(pred_4_z$pred_mean, pred_4_z$quantity)
# Graphical Comparison
pred_3_z$x_axis = seq(1:168)
pred_4_z$x_axis = seq(1:168)
fig1.4 = pred_3_z %>% ggplot() +
geom_point(aes(x=x_axis, y=quantity), color = 'dodgerblue') +
geom_line(aes(x=x_axis, y=pred_mean)) +
geom_ribbon(aes(x=x_axis,
ymin = pred_lPI,
ymax = pred_hPI),
alpha = 0.7)+
ggtitle("Model1 + Method3")
fig2.4 = pred_4_z %>% ggplot() +
geom_point(aes(x=x_axis, y=quantity), color = 'dodgerblue') +
geom_line(aes(x=x_axis, y=pred_mean)) +
geom_ribbon(aes(x=x_axis,
ymin = pred_lPI,
ymax = pred_hPI),
alpha = 0.7) +
ggtitle("Model2 + Method3")
grid.arrange(fig1.2, fig2.2, fig1.3, fig2.3, fig1.4, fig2.4, nrow=3)
cat("The RMSE of model_1 + method_2 is",
RMSE(test_2$pred_1_mean, test_2$quantity),"\n")
cat("The RMSE of model_2 + method_2 is",
RMSE(test_2$pred_2_mean, test_2$quantity),"\n")
cat("The RMSE of model_1 + method_3 is",
RMSE(pred_3_z$pred_mean, pred_3_z$quantity),"\n")
cat("The RMSE of model_2 + method_3 is",
RMSE(pred_4_z$pred_mean, pred_4_z$quantity))
fig2.4 +
theme(
panel.background = element_rect(fill = "transparent") # bg of the panel
, plot.background = element_rect(fill = "transparent", color = NA) # bg of the plot
, panel.grid.major = element_blank() # get rid of major grid
#, panel.grid.minor = element_blank() # get rid of minor grid
#, legend.background = element_rect(fill = "transparent") # get rid of legend bg
, legend.box.background = element_rect(fill = "transparent") # get rid of legend panel bg
)
save.image("C:/Users/Administrator.BV550197140001/Desktop/Statistical-Rethinking/Final Project/RUNALL.RData")
